{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "certification",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfsUEFb658CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test model button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# Getting Started Question\n",
        "#\n",
        "# Given this data, train a neural network to match the xs to the ys\n",
        "# So that a predictor for a new value of X will give a float value\n",
        "# very close to the desired answer\n",
        "# i.e. print(model.predict([10.0])) would give a satisfactory result\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "    ys = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "\n",
        "    model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "    model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "    model.fit(xs, ys, epochs=500)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpQk976O63pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# Basic Datasets Question\n",
        "#\n",
        "# Create a classifier for the Fashion MNIST dataset\n",
        "# Note that the test will expect it to classify 10 classes and that the\n",
        "# input shape should be the native size of the Fashion MNIST dataset which is\n",
        "# 28x28 monochrome. Do not resize the data. Your input layer should accept\n",
        "# (28,28) as the input shape only. If you amend this, the tests will fail.\n",
        "#\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    (training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "    training_images = training_images / 255.0\n",
        "    test_images = test_images / 255.0\n",
        "    model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "    model.compile(optimizer=tf.optimizers.Adam(),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(training_images, training_labels, epochs=5)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6k267pjAVJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# Computer vision with CNNs\n",
        "#\n",
        "# Create and train a classifier for horses or humans using the provided data.\n",
        "# Make sure your final layer is a 1 neuron, activated by sigmoid as shown.\n",
        "#\n",
        "# The test will use images that are 300x300 with 3 bytes color depth so be sure to\n",
        "# design your neural network accordingly\n",
        "\n",
        "import tensorflow as tf\n",
        "import urllib\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
        "    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
        "    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\n",
        "    local_zip = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/horse-or-human/')\n",
        "    zip_ref.close()\n",
        "    urllib.request.urlretrieve(_TEST_URL, 'validation-horse-or-human.zip')\n",
        "    local_zip = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/validation-horse-or-human/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "        # Your code here. Should at least have a rescale. Other parameters can help with overfitting.\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255\n",
        "        # Your Code here\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory('tmp/horse-or-human/',\n",
        "                                                        batch_size=2,\n",
        "                                                        class_mode='binary',\n",
        "                                                        target_size=(300, 300)\n",
        "                                                        # Your Code Here\n",
        "                                                        )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory('tmp/validation-horse-or-human/',\n",
        "                                                                  batch_size=2,\n",
        "                                                                  class_mode='binary',\n",
        "                                                                  target_size=(300, 300)\n",
        "                                                                  # Your Code Here\n",
        "                                                                  )\n",
        "\n",
        "    # Import the inception model\n",
        "    from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "    # Create an instance of the inception model from the local pre-trained weights\n",
        "    url_model='https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "    urllib.request.urlretrieve(url_model,'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "    local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "    pre_trained_model = InceptionV3(input_shape=(300, 300, 3),\n",
        "                                    include_top=False,\n",
        "                                    weights=None)\n",
        "\n",
        "    pre_trained_model.load_weights(local_weights_file)\n",
        "    last_layer = pre_trained_model.get_layer('mixed7')\n",
        "    last_output = last_layer.output\n",
        "\n",
        "    class myCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if (logs.get('acc') > 0.97):\n",
        "                print(\"\\nReached 97.0% accuracy so cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "    from tensorflow.keras.optimizers import RMSprop\n",
        "    from tensorflow.keras import layers\n",
        "    from tensorflow.keras import Model\n",
        "    # Flatten the output layer to 1 dimension\n",
        "    x = layers.Flatten()(last_output)\n",
        "    # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    # Add a dropout rate of 0.2\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    # Add a final sigmoid layer for classification\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    callbacks = myCallback()\n",
        "    model = Model(pre_trained_model.input, x)\n",
        "\n",
        "    model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    model.fit_generator(train_generator,\n",
        "                        epochs=3,\n",
        "                        verbose=1,\n",
        "                        validation_data=validation_generator,\n",
        "                        steps_per_epoch=20,\n",
        "                        validation_steps=20,\n",
        "                        callbacks=[callbacks]\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "    # NOTE: If training is taking a very long time, you should consider setting the batch size\n",
        "    # appropriately on the generator, and the steps per epoch in the model.fit() function.\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0N__WIPzQeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('mymodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZm9EiczW_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMihWFENz2R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('mymodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4YTQ8-0Izq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asSH9pXFIGif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    with open(\"sarcasm.json\", 'r') as f:\n",
        "      datastore = json.load(f)\n",
        "    urls = []\n",
        "    for item in datastore:\n",
        "        sentences.append(item['headline'])\n",
        "        labels.append(item['is_sarcastic'])\n",
        "    \n",
        "    training_sentences = sentences[0:training_size]\n",
        "    testing_sentences = sentences[training_size:]\n",
        "    training_labels = labels[0:training_size]\n",
        "    testing_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "            tf.keras.layers.Dense(24, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "            # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    num_epochs = 10\n",
        "    training_padded = np.array(training_padded)\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_padded = np.array(testing_padded)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "    history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lE6_9b0iS8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "model.save('model.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'model.h5'})\n",
        "model_file.SetContentFile('model.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-GPUnAIUDRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "# QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict sunspot activity using\n",
        "# the Sunspots.csv dataset.\n",
        "#\n",
        "# Your neural network must have an MAE of 0.12 or less on the normalized dataset\n",
        "# for top marks.\n",
        "#\n",
        "# Code for normalizing the data is provided and should not be changed.\n",
        "#\n",
        "# At the bottom of this file, we provide  some testing\n",
        "# code in case you want to check your model.\n",
        "\n",
        "# Note: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure.\n",
        "\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    series = np.array(sunspots) # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # This is the normalization function\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # The data should be split into training and validation sets at time step 3000\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 3000\n",
        "\n",
        "    time_train = time[:split_time] # YOUR CODE HERE\n",
        "    x_train = series[:split_time] # YOUR CODE HERE\n",
        "    time_valid = time[split_time:] # YOUR CODE HERE\n",
        "    x_valid = series[split_time:] # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n",
        "      strides=1, padding=\"causal\",\n",
        "      activation=\"relu\",\n",
        "      input_shape=[None, 1]),\n",
        "      tf.keras.layers.LSTM(4, input_shape=(1, 1)),\n",
        "      # YOUR CODE HERE. Whatever your first layer is, the input shape will be [None,1] when using the Windowed_dataset above, depending on the layer type chosen\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    # PLEASE NOTE IF YOU SEE THIS TEXT WHILE TRAINING -- IT IS SAFE TO IGNORE\n",
        "    # BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
        "    # \t [[{{node IteratorGetNext}}]]\n",
        "    #\n",
        "    optimizer = 'adam'\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=[\"mae\"])\n",
        "    history = model.fit(train_set, epochs=50,)\n",
        "\n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel5.h5\")\n",
        "\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "#def model_forecast(model, series, window_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(32).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "\n",
        "#window_size = # YOUR CODE HERE\n",
        "#rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "#rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
        "\n",
        "#result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "\n",
        "## To get the maximum score, your model must have an MAE OF .12 or less.\n",
        "## When you Submit and Test your model, the grading infrastructure\n",
        "## converts the MAE of your model to a score from 0 to 5 as follows:\n",
        "\n",
        "#test_val = 100 * result\n",
        "#score = math.ceil(17 - test_val)\n",
        "#if score > 5:\n",
        "#    score = 5\n",
        "\n",
        "#print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_L6o0lfpVk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}